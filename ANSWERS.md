# Ответы на задание I

1. С какими проблемами можно столкнуться при таком способе хранения состояния обработки входных данных?
	* race condition
	* чрезмерная нагрузка на сервис Operation


2. Что предпринять, если происходит “гонка” обновления данных операции в Operation и часть данных может перезаписываться старыми?
	* блокировки
	* транзакции


3. Как решить проблему с большим количеством UPDATE операций в PostgreSQL? И на каком уровне стоит ее решать?
    * переписать мониторинг на режим вытягивания: Operation сам ходит в нужные сервисы и запрашивает у них историю изменения операций
    * можно "копить" запросы на обновление и проводить BULK UPDATE раз в n времени
    * можно использовать партицирование БД
   Зависит от ситуации, но, если некоторое отставание статуса операции в Operation не критично и есть ресурсы, я бы выбрал первый.  
	

4. Как можно спроектировать сервис для отслеживания состояния обработки входных JSON по всему pipeline, чтобы избежать подобных проблем? Какие минусы у вашего варианта реализации такого сервиса?

    Можно использовать pull мониторинг, как описал выше.
    Минусы:
    * данные в Operation могут отставать, т.к. обновляются с определенной частотой
    * нужно реализовать хранение данных по операциям в сервисах и API для отдачи этих данных



